{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the metadata, feature space, and genres are read in and stored in the \"songs\" dictionary by `track_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from random import sample as rand_sample\n",
    "from random import choice as rand_choice\n",
    "from sklearn.cluster import KMeans\n",
    "from maps import FrozenMap, FixedKeyMap\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file):\n",
    "    '''\n",
    "    description\n",
    "\n",
    "    :param file:        file we're reading in with the data (.csv)\n",
    "\n",
    "    :return             a 'songs' dictionary\n",
    "                        key: track_id\n",
    "                        value(s): a 'data' dictionary, which contains:\n",
    "                            metadata,\n",
    "                            label,\n",
    "                            features,\n",
    "                            genres\n",
    "    '''\n",
    "    \n",
    "    songs = {}\n",
    "\n",
    "    with open(file, 'r') as f:\n",
    "\n",
    "        for song in f:\n",
    "            \n",
    "            line = song.split(',')\n",
    "                        \n",
    "            # ignore first line + ensure that label exists\n",
    "            if line[0] != 'label' and line[0]:\n",
    "                label = int(line[0])\n",
    "                metadata, data = {}, {}\n",
    "                genres = []\n",
    "                track_id = line[1]\n",
    "                \n",
    "                features = {}\n",
    "                features['artist_popularity'] = line[4]\n",
    "                features['artist_followers'] = line[5]\n",
    "\n",
    "                metadata['artist_id'] = line[2]\n",
    "                metadata['artist_name'] = line[3]\n",
    "                \n",
    "                # check if genre field has multiple genres or just one\n",
    "                if line[6]:\n",
    "                    genres.append(line[6][1:]) if '\"' in line[6] else genres.append(line[6])\n",
    "\n",
    "                count = 0\n",
    "                \n",
    "                # if '\"' present or next value is a string only containing alphabets,\n",
    "                # then add to genres list. set count to i+1 when you reach last genre\n",
    "                for i in range(7, len(line)):\n",
    "                    if '\"' in line[i]:\n",
    "                        genres.append(line[i][:-1])\n",
    "                        count = i+1\n",
    "                        break\n",
    "                    if line[i].isalpha():\n",
    "                        genres.append(line[i])\n",
    "                \n",
    "                # single or no genres, get all other features\n",
    "                if count == 0:\n",
    "                    features['instrumentalness'] = float(line[7])\n",
    "                    features['duration_ms'] = float(line[8])\n",
    "                    features['time_signature'] = float(line[9])\n",
    "                    features['acousticness'] = float(line[10])\n",
    "                    features['speechiness'] = float(line[11])\n",
    "                    features['energy'] = float(line[12])\n",
    "                    features['loudness'] = float(line[13])\n",
    "                    features['tempo'] = float(line[14])\n",
    "                    features['key'] = float(line[15])\n",
    "                    features['valence'] = float(line[16])\n",
    "                    features['danceability'] = float(line[17])\n",
    "                    features['liveness'] = float(line[18][:-1])\n",
    "                \n",
    "                # multiple genres, get all other features using count\n",
    "                else:\n",
    "                    features['instrumentalness'] = float(line[count])\n",
    "                    features['duration_ms'] = float(line[count+1])\n",
    "                    features['time_signature'] = float(line[count+2])\n",
    "                    features['acousticness'] = float(line[count+3])\n",
    "                    features['speechiness'] = float(line[count+4])\n",
    "                    features['energy'] = float(line[count+5])\n",
    "                    features['loudness'] = float(line[count+6])\n",
    "                    features['tempo'] = float(line[count+7])\n",
    "                    features['key'] = float(line[count+8])\n",
    "                    features['valence'] = float(line[count+9])\n",
    "                    features['danceability'] = float(line[count+10])\n",
    "                    features['liveness'] = float(line[count+11][:-1])\n",
    "                \n",
    "                # add metadata, features, genres, and label to data\n",
    "                data['metadata'] = metadata\n",
    "                data['features'] = features\n",
    "                data['genres'] = genres\n",
    "                data['label'] = label\n",
    "                \n",
    "                # add data to songs by track_id\n",
    "                songs[track_id] = data\n",
    "\n",
    "    return songs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where genres are read in, with each genre getting a unique `genre_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isFloat(string):\n",
    "    '''\n",
    "    description\n",
    "\n",
    "    :param string:      the string we're testing\n",
    "\n",
    "    :return             True if the string is a float, False otherwise\n",
    "    '''\n",
    "    try:\n",
    "        float(string)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def read_genres(file):\n",
    "    '''\n",
    "    description\n",
    "    \n",
    "    :param file:        file we're reading in with the data (.csv)\n",
    "    \n",
    "    :return             a 'genres' dictionary\n",
    "                        key: genre\n",
    "                        value: unique genre_id\n",
    "    '''\n",
    "    genre_mapping = {}\n",
    "    genre_id = 0\n",
    "    \n",
    "    with open(file, 'r') as f:\n",
    "\n",
    "        for song in f:\n",
    "\n",
    "            line = song.split(',')\n",
    "\n",
    "            if line[0]:\n",
    "\n",
    "                genres = []\n",
    "                \n",
    "                # check if genres field is empty or contains one/multiple genres\n",
    "                if line[6]:\n",
    "                    genres.append(line[6][1:]) if '\"' in line[6] else genres.append(line[6])\n",
    "\n",
    "                count = 0\n",
    "                \n",
    "                # account for multiple genres\n",
    "                for i in range(7, len(line)):\n",
    "                    if '\"' in line[i]:\n",
    "                        genres.append(line[i][:-1])\n",
    "                        count = i+1\n",
    "                        break\n",
    "                    if line[i].isalpha():\n",
    "                        genres.append(line[i])\n",
    "                \n",
    "                # for genres in the genres list, ensure that the genre is not a float, create unique genre_id\n",
    "                for genre in genres:\n",
    "                    if (genre not in genre_mapping) and ('\"' not in genre) and (not isFloat(genre)):\n",
    "                        genre_mapping[genre] = genre_id\n",
    "                        genre_id += 1\n",
    "        \n",
    "    return genre_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the data is actually read in. We read in the `likes.csv` + `dislikes.csv` data, and merge them into one dictionary, `song_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_song_data = FrozenMap(read_data('data.csv'))\n",
    "genres = FrozenMap(read_genres('data.csv'))\n",
    "default_feature_names = tuple(next(iter(default_song_data.values()))['features'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('artist_popularity',\n",
       " 'artist_followers',\n",
       " 'instrumentalness',\n",
       " 'duration_ms',\n",
       " 'time_signature',\n",
       " 'acousticness',\n",
       " 'speechiness',\n",
       " 'energy',\n",
       " 'loudness',\n",
       " 'tempo',\n",
       " 'key',\n",
       " 'valence',\n",
       " 'danceability',\n",
       " 'liveness')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " likes: 1919 \n",
      " dislikes: 3790\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "l, dl = 0, 0\n",
    "for x in default_song_data.values():\n",
    "    if x['label'] == 1: l += 1\n",
    "    else: dl += 1\n",
    "print(' likes:', l, '\\n', 'dislikes:', dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 artist_genres\n",
      "1 instrumentalness\n",
      "2 acousticness\n",
      "3 speechiness\n",
      "4 energy\n",
      "5 tempo\n",
      "6 key\n",
      "7 valence\n",
      "8 danceability\n",
      "9 uk contemporary r&b\n",
      "10 future garage\n",
      "11 house\n",
      "12 nu disco\n",
      "13 christmas\n",
      "14 funk\n",
      "15 motown\n",
      "16 soul\n",
      "17 dub\n",
      "18 reggae\n",
      "19 roots reggae\n",
      "20 hindustani classical\n",
      "21 indian classical\n",
      "22 bass music\n",
      "23 microhouse\n",
      "24 indie jazz\n",
      "25 dance pop\n",
      "26 pop\n",
      "27 chicago house\n",
      "28 tech house\n",
      "29 alternative dance\n",
      "30 indietronica\n",
      "31 new rave\n",
      "32 afro house\n",
      "33 filter house\n",
      "34 drum and bass\n",
      "35 drumfunk\n",
      "36 jungle\n",
      "37 neurofunk\n",
      "38 uk garage\n",
      "39 world fusion\n",
      "40 deep house\n",
      "41 minimal tech house\n",
      "42 ambient\n",
      "43 drone\n",
      "44 electra\n",
      "45 fluxwork\n",
      "46 mandible\n",
      "47 downtempo\n",
      "48 electronic\n",
      "49 vaporwave\n",
      "50 wonky\n",
      "51 acid idm\n",
      "52 big beat\n",
      "53 breakbeat\n",
      "54 deep soul house\n",
      "55 hip hop\n",
      "56 rap\n",
      "57 underground hip hop\n",
      "58 aussietronica\n",
      "59 french indietronica\n",
      "60 british invasion\n",
      "61 merseybeat\n",
      "62 rock\n",
      "63 deep groove house\n",
      "64 bow pop\n",
      "65 focus\n",
      "66 neoclassical\n",
      "67 adult standards\n",
      "68 folk\n",
      "69 soft rock\n",
      "70 tropical house\n",
      "71 deep melodic euro house\n",
      "72 substep\n",
      "73 balearic\n",
      "74 minimal dub\n",
      "75 art pop\n",
      "76 experimental\n",
      "77 industrial\n",
      "78 shoegaze\n",
      "79 uk post-punk\n",
      "80 chillwave\n",
      "81 techno\n",
      "82 vapor soul\n",
      "83 outsider house\n",
      "84 deep minimal techno\n",
      "85 electrofox\n",
      "86 detroit techno\n",
      "87 electro\n",
      "88 indian indie\n",
      "89 destroy techno\n",
      "90 electro house\n",
      "91 float house\n",
      "92 alternative hip hop\n",
      "93 lo-fi house\n",
      "94 brostep\n",
      "95 conscious hip hop\n",
      "96 west coast rap\n",
      "97 deep disco house\n",
      "98 electronic trap\n",
      "99 vapor twitch\n",
      "100 australian indie\n",
      "101 french indie pop\n",
      "102 footwork\n",
      "103 alternative rock\n",
      "104 scottish rock\n",
      "105 deep liquid\n",
      "106 liquid funk\n",
      "107 shimmer pop\n",
      "108 electropop\n",
      "109 hauntology\n",
      "110 chamber pop\n",
      "111 acid jazz\n",
      "112 scottish singer-songwriter\n",
      "113 grime\n",
      "114 modern rock\n",
      "115 afrobeat\n",
      "116 afropop\n",
      "117 world\n",
      "118 tribal house\n",
      "119 dub techno\n",
      "120 britpop\n",
      "121 madchester\n",
      "122 synthpop\n",
      "123 ninja\n",
      "124 trip hop\n",
      "125 chamber psych\n",
      "126 modern alternative rock\n",
      "127 acid house\n",
      "128 austindie\n",
      "129 hungarian rock\n",
      "130 german techno\n",
      "131 metropopolis\n",
      "132 abstract beats\n",
      "133 chillhop\n",
      "134 disco\n",
      "135 vintage french electronic\n",
      "136 latin afrobeat\n",
      "137 schranz\n",
      "138 indie napoletano\n",
      "139 italian alternative\n",
      "140 british jazz\n",
      "141 canadian soundtrack\n",
      "142 post-rock\n",
      "143 future funk\n",
      "144 dubstep\n",
      "145 escape room\n",
      "146 lgbtq+ hip hop\n",
      "147 dark post-punk\n",
      "148 canadian contemporary r&b\n",
      "149 edm\n",
      "150 progressive house\n",
      "151 nu jazz\n",
      "152 indie electro-pop\n",
      "153 electroclash\n",
      "154 neo-synthpop\n",
      "155 swedish electronic\n",
      "156 vocal house\n",
      "157 melancholia\n",
      "158 minimal techno\n",
      "159 disco house\n",
      "160 retro electro\n",
      "161 electro jazz\n",
      "162 hardcore techno\n",
      "163 ballet class\n",
      "164 forro\n",
      "165 samba\n",
      "166 russian electronic\n",
      "167 lounge\n",
      "168 glitch\n",
      "169 indie r&b\n",
      "170 australian alternative rock\n",
      "171 psychedelic rock\n",
      "172 shiver pop\n",
      "173 album rock\n",
      "174 metal\n",
      "175 nwobhm\n",
      "176 wrestling\n",
      "177 new wave pop\n",
      "178 urban contemporary\n",
      "179 hip pop\n",
      "180 contemporary jazz\n",
      "181 soundtrack\n",
      "182 compositional ambient\n",
      "183 malaysian indie\n",
      "184 post-disco\n",
      "185 deep disco\n",
      "186 australian dance\n",
      "187 stomp and holler\n",
      "188 dance-punk\n",
      "189 broken beat\n",
      "190 hands up\n",
      "191 bassline\n",
      "192 turkish electronic\n",
      "193 uk alternative hip hop\n",
      "194 vapor pop\n",
      "195 grunge pop\n",
      "196 chillstep\n",
      "197 wave\n",
      "198 swedish synthpop\n",
      "199 bebop\n",
      "200 jazz\n",
      "201 vocal jazz\n",
      "202 re:techno\n",
      "203 acid techno\n",
      "204 speed garage\n",
      "205 mexican indie\n",
      "206 minimal dubstep\n",
      "207 ectofolk\n",
      "208 lilith\n",
      "209 singer-songwriter\n",
      "210 hard house\n",
      "211 zolo\n",
      "212 vienna indie\n",
      "213 deep euro house\n",
      "214 ecuadorian indie\n",
      "215 latintronica\n",
      "216 portland indie\n",
      "217 post-teen pop\n",
      "218 latin classical\n",
      "219 scorecore\n",
      "220 video game music\n",
      "221 lo-fi beats\n",
      "222 southern hip hop\n",
      "223 minimal melodic techno\n",
      "224 uk hip hop\n",
      "225 experimental pop\n",
      "226 bubble trance\n",
      "227 trance\n",
      "228 drift\n",
      "229 warm drone\n",
      "230 icelandic indie\n",
      "231 icelandic pop\n",
      "232 edmonton indie\n",
      "233 dark minimal techno\n",
      "234 russian jazz\n",
      "235 digital hardcore\n",
      "236 alternative metal\n",
      "237 djent\n",
      "238 progressive metal\n",
      "239 bristol indie\n",
      "240 deep liquid bass\n",
      "241 deep progressive trance\n",
      "242 british soundtrack\n",
      "243 german soundtrack\n",
      "244 uplifting trance\n",
      "245 fidget house\n",
      "246 abstract\n",
      "247 chilean indie\n",
      "248 turntablism\n",
      "249 ukrainian experimental\n",
      "250 bass trap\n",
      "251 moombahton\n",
      "252 dark jazz\n",
      "253 quebec indie\n",
      "254 nu gaze\n",
      "255 neo soul\n",
      "256 big room\n",
      "257 progressive electro house\n",
      "258 r&b\n",
      "259 bass trip\n",
      "260 polynesian pop\n",
      "261 rock kapak\n",
      "262 cumbia funk\n",
      "263 canadian folk\n",
      "264 saskatchewan indie\n",
      "265 icelandic electronic\n",
      "266 boston hardcore\n",
      "267 mathcore\n",
      "268 metalcore\n",
      "269 sludge metal\n",
      "270 canadian pop\n",
      "271 ambient idm\n",
      "272 screamo\n",
      "273 french rock\n",
      "274 belgian pop\n",
      "275 rock steady\n",
      "276 alternative pop\n",
      "277 gbvfi\n",
      "278 slow core\n",
      "279 soul jazz\n",
      "280 punk blues\n",
      "281 japanese jazz\n",
      "282 danish electro-pop\n",
      "283 minimal\n",
      "284 catstep\n",
      "285 complextro\n",
      "286 jazz rap\n",
      "287 permanent wave\n",
      "288 punk\n",
      "289 darkstep\n",
      "290 black sludge\n",
      "291 canadian hip hop\n",
      "292 indie garage rock\n",
      "293 neo-psychedelic\n",
      "294 ghettotech\n",
      "295 canadian contemporary country\n",
      "296 lift kit\n",
      "297 contemporary country\n",
      "298 country\n",
      "299 country road\n",
      "300 modern country rock\n",
      "301 redneck\n",
      "302 canadian country\n",
      "303 trap music\n",
      "304 vapor trap\n",
      "305 viral pop\n",
      "306 boy band\n",
      "307 talent show\n",
      "308 pop rap\n",
      "309 emo rap\n",
      "310 dirty south rap\n",
      "311 east coast hip hop\n",
      "312 drill\n",
      "313 europop\n",
      "314 hollywood\n",
      "315 australian pop\n",
      "316 desi\n",
      "317 filmi\n",
      "318 sufi\n",
      "319 classical tenor\n",
      "320 operatic pop\n",
      "321 strut\n",
      "322 pop rock\n",
      "323 deep talent show\n",
      "324 australian country\n",
      "325 texas country\n",
      "326 modern southern rock\n",
      "327 traditional country\n",
      "328 country dawn\n",
      "329 nashville sound\n",
      "330 traditional folk\n",
      "331 outlaw country\n",
      "332 country rock\n",
      "333 bluegrass\n",
      "334 progressive bluegrass\n",
      "335 red dirt\n",
      "336 alternative country\n",
      "337 country rap\n",
      "338 modern uplift\n",
      "339 roots americana\n",
      "340 southern rock\n",
      "341 british country\n",
      "342 uk americana\n",
      "343 deep contemporary country\n",
      "344 eurodance\n",
      "345 australian indigenous\n",
      "346 australian rock\n",
      "347 alberta country\n",
      "348 alaska indie\n",
      "349 western swing\n",
      "350 cowboy western\n",
      "351 trap soul\n",
      "352 etherpop\n",
      "353 latin\n",
      "354 reggaeton\n",
      "355 irish rock\n",
      "356 tropical pop edm\n",
      "357 electropowerpop\n",
      "358 mashup\n",
      "359 tracestep\n",
      "360 hardstyle\n",
      "361 sky room\n",
      "362 deep uplifting trance\n",
      "363 belgian dance\n",
      "364 emo\n",
      "365 trancecore\n",
      "366 deep big room\n",
      "367 dutch house\n",
      "368 ambient fusion\n",
      "369 laboratorio\n",
      "370 indie pop\n",
      "371 deep hardstyle\n",
      "372 bubblegum dance\n",
      "373 hip house\n",
      "374 indie pop rap\n",
      "375 rap rock\n",
      "376 deep tropical house\n",
      "377 dutch pop\n",
      "378 berlin minimal techno\n",
      "379 electro trash\n",
      "380 uk funky\n",
      "381 funky tech house\n",
      "382 bmore\n",
      "383 dancehall\n",
      "384 filthstep\n",
      "385 indie poptimism\n",
      "386 swedish electropop\n",
      "387 swedish soul\n",
      "388 indie folk\n",
      "389 zapstep\n",
      "390 anthem worship\n",
      "391 ccm\n",
      "392 worship\n",
      "393 gospel\n",
      "394 naija worship\n",
      "395 christian hip hop\n",
      "396 christian trap\n",
      "397 acoustic pop\n",
      "398 indiecoustica\n",
      "399 idol\n",
      "400 latin worship\n",
      "401 christian music\n",
      "402 channel pop\n",
      "403 deep christian rock\n",
      "404 south african pop\n",
      "405 latin christian\n",
      "406 world worship\n",
      "407 adoracao\n",
      "408 brazilian gospel\n",
      "409 christian uplift\n",
      "410 alternative ccm\n",
      "411 rap cristiano\n",
      "412 christian rock\n",
      "413 praise\n",
      "414 canadian ccm\n",
      "415 indie rockism\n",
      "416 neo mellow\n",
      "417 christian dance\n",
      "418 christian alternative rock\n",
      "419 post-grunge\n",
      "420 indonesian pop\n",
      "421 malaysian pop\n",
      "422 christian relaxative\n",
      "423 louvor\n",
      "424 southern gospel\n",
      "425 country gospel\n",
      "426 christelijk\n",
      "427 german worship\n",
      "428 indonesian worship\n",
      "429 teen pop\n",
      "430 deep ccm\n",
      "431 russian ccm\n",
      "432 mod revival\n",
      "433 soul flow\n",
      "434 folk punk\n",
      "435 g funk\n",
      "436 gospel rap\n",
      "437 deep freestyle\n",
      "438 deep underground hip hop\n",
      "439 underground rap\n",
      "440 abstract hip hop\n",
      "441 nursery\n",
      "442 children's music\n",
      "443 vbs\n",
      "444 dreamo\n",
      "445 poetry\n",
      "446 reading\n",
      "447 drama\n",
      "448 vintage radio show\n",
      "449 cabaret\n",
      "450 german literature\n",
      "451 hoerspiel\n",
      "452 symphonic rock\n",
      "453 celtic\n",
      "454 irish folk\n",
      "455 healing\n",
      "456 motivation\n",
      "457 show tunes\n",
      "458 k-pop\n",
      "459 korean pop\n",
      "460 korean r&b\n",
      "461 j-pop\n",
      "462 k-indie\n",
      "463 classic russian rock\n",
      "464 diva house\n",
      "465 k-hop\n",
      "466 j-metal\n",
      "467 deep pop r&b\n",
      "468 battle rap\n",
      "469 cantopop\n",
      "470 canterbury scene\n",
      "471 k-rock\n",
      "472 deep latin alternative\n",
      "473 deep southern trap\n",
      "474 crunk\n",
      "475 candy pop\n",
      "476 deep funk carioca\n",
      "477 tropical\n",
      "478 detroit hip hop\n",
      "479 gangster rap\n",
      "480 indie rock\n",
      "481 trap latino\n",
      "482 reggaeton flow\n",
      "483 pop punk\n",
      "484 halloween\n",
      "485 sertanejo pop\n",
      "486 sertanejo universitario\n",
      "487 opm\n",
      "488 pinoy indie\n",
      "489 colombian hip hop\n",
      "490 baile pop\n",
      "491 cumbia paraguaya\n",
      "492 cumbia villera\n",
      "493 swedish idol pop\n",
      "494 swedish pop\n",
      "495 panamanian pop\n",
      "496 blues\n",
      "497 traditional blues\n",
      "498 salsa\n",
      "499 brega funk\n",
      "500 horror punk\n",
      "501 skate punk\n",
      "502 deep swedish hip hop\n",
      "503 bubblegum pop\n",
      "504 underground power pop\n",
      "505 brill building pop\n",
      "506 freak folk\n",
      "507 preverb\n",
      "508 dance rock\n",
      "509 comic\n",
      "510 rockabilly\n",
      "511 broadway\n",
      "512 jumpstyle\n"
     ]
    }
   ],
   "source": [
    "for k in genres:\n",
    "    print(genres[k], k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('artist_popularity',\n",
       " 'artist_followers',\n",
       " 'instrumentalness',\n",
       " 'duration_ms',\n",
       " 'time_signature',\n",
       " 'acousticness',\n",
       " 'speechiness',\n",
       " 'energy',\n",
       " 'loudness',\n",
       " 'tempo',\n",
       " 'key',\n",
       " 'valence',\n",
       " 'danceability',\n",
       " 'liveness')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_features(data, discard):\n",
    "    '''\n",
    "    Filters out features from data. Does not modify passed-in object (creates a copy).\n",
    "    \n",
    "    :param data:        dict of data, same format as song_data\n",
    "    :param discard:     feature names to discard\n",
    "                        \n",
    "    :return             copy of data, with filtered features\n",
    "    '''\n",
    "    out = dict(data)\n",
    "    \n",
    "    for id_ in data:\n",
    "        out[id_] = deepcopy(out[id_])\n",
    "        out[id_]['features'] = { k:v for k, v in out[id_]['features'].items() if k not in discard }\n",
    "    \n",
    "    return FrozenMap(out)\n",
    "\n",
    "def split_data(data, p):\n",
    "    '''\n",
    "    Splits data into training and validation sets for simple classification.\n",
    "    \n",
    "    :param data:    complete labeled data\n",
    "    :param p:       proportion of data to use for validation\n",
    "                    \n",
    "    :return         train_data, validation_data\n",
    "    '''\n",
    "    validation_ids = rand_sample(list(data), int(len(data) * p))\n",
    "    \n",
    "    validation_data = {k:data[k] for k in validation_ids}\n",
    "    train_data = {k:v for k, v in data.items() if k not in validation_data}\n",
    "\n",
    "    return FrozenMap(train_data), FrozenMap(validation_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3123.394736313185\n",
      "3 2481.716632257166\n",
      "4 2152.068476365455\n",
      "5 1944.165017191881\n",
      "6 1824.5853227488217\n",
      "7 1711.4623613593121\n",
      "8 1612.5531653294152\n",
      "9 1536.3092762289932\n",
      "10 1474.2539202926469\n"
     ]
    }
   ],
   "source": [
    "def test_cluster_size(data, max_cluster):\n",
    "    '''\n",
    "    description\n",
    "\n",
    "    :param data:        the data, obtained from read_data()\n",
    "    :param max_cluster: the max number of clusters to km.inertia_ on\n",
    "\n",
    "    :return             None (prints km_inertia for each number of clusters)\n",
    "    '''\n",
    "    train_data = {k:v for k, v in data.items()}\n",
    "    x_train = np.array([list(x['features'].values()) for x in train_data.values()])\n",
    "    \n",
    "    for i in range(2, max_cluster+1):\n",
    "        km = KMeans(i, init='random', max_iter=300, random_state=0, n_init=30)\n",
    "        km.fit(x_train)\n",
    "        print(i, km.inertia_)\n",
    "\n",
    "test_cluster_size(default_song_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kmeans_clusters(data, n_clusters, songs_by_cluster={}):\n",
    "    '''\n",
    "    Assigns songs in data to :n_clusters: unique clusters.\n",
    "    \n",
    "    FUTURE: Refactor feature engineering such that there data\n",
    "            isn't copied wholesale. E.g., implement multiple\n",
    "            kinds of features or better feature filtering.\n",
    "            This will require changes to Classifier.\n",
    "\n",
    "    :param data:        the data, obtained from read_data()\n",
    "    :param n_clusters:  the number of clusters used in K-means\n",
    "\n",
    "    :return             deep copy of data with cluster property and clusters as boolean features\n",
    "                        dict, n_clusters:cluster:ids_of_songs_in_cluster\n",
    "    '''\n",
    "    \n",
    "    data = dict(data)\n",
    "    \n",
    "    if n_clusters in songs_by_cluster.keys(): raise ValueError(str(n_clusters) + ' clusters already computed')\n",
    "    \n",
    "    songs_by_cluster[n_clusters] = { i:set() for i in range(n_clusters) }\n",
    "        \n",
    "    train_data, track_ids = {k:v for k, v in data.items()}, list(data)\n",
    "\n",
    "    x_train = np.array([list(x['features'].values()) for x in train_data.values()])\n",
    "    y_train = [x['label'] for x in train_data.values()]\n",
    "\n",
    "    km = KMeans(n_clusters, init='random', max_iter=300, random_state=0, n_init=30)\n",
    "    km.fit(x_train)\n",
    "\n",
    "    cluster_map = pd.DataFrame()\n",
    "\n",
    "    cluster_map['data'], cluster_map['cluster'] = x_train.tolist(), km.labels_\n",
    "    cluster_map['label'], cluster_map['track_id'] = y_train, track_ids\n",
    "\n",
    "    for track_id in track_ids:\n",
    "        \n",
    "        data[track_id] = deepcopy(data[track_id])\n",
    "        \n",
    "        cluster = cluster_map[cluster_map['track_id'] == track_id]['cluster'].tolist()[0]\n",
    "        \n",
    "        songs_by_cluster[n_clusters][cluster].add(track_id)\n",
    "        \n",
    "        data[track_id]['cluster'] = cluster\n",
    "        for i in range(n_clusters):\n",
    "            data[track_id]['features']['c'+str(i)] = 1 if cluster == i else 0\n",
    "    \n",
    "    return FrozenMap(data), FrozenMap(songs_by_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLUSTERS = 10\n",
    "\n",
    "clustered_song_data, songs_by_cluster = get_kmeans_clusters(default_song_data, NUM_CLUSTERS)\n",
    "# clustered_song_data\n",
    "# songs_by_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if set(next(iter(default_song_data.values()))['features'].keys()) == set(next(iter(clustered_song_data.values()))['features'].keys()):\n",
    "    raise ValueError('Default features messed up.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUPPORTED_ALGS = ['svc', 'lsvc', 'sgd']\n",
    "ONLINE_ALGS = ['sgd']\n",
    "AL_STRATS = ['random', 'uncertainty']\n",
    "CLASSES = [0, 1]\n",
    "\n",
    "def get_xy(song_data, ids=None):\n",
    "    if ids: song_data = {k:song_data[k] for k in ids}\n",
    "    return [x['features'] for x in song_data.values()], [x['label'] for x in song_data.values()]\n",
    "\n",
    "def get_default_vectorizer(song_data_x):\n",
    "    vect = DictVectorizer(sort=True)\n",
    "    return vect.fit(song_data_x)\n",
    "\n",
    "def get_default_learner(algorithm):\n",
    "    if algorithm == 'svc':\n",
    "            return SVC(gamma='auto') # defaults\n",
    "        \n",
    "    elif algorithm == 'lsvc':\n",
    "        return LinearSVC(loss='hinge', penalty='l2') # defaults\n",
    "\n",
    "    elif algorithm == 'sgd':\n",
    "        return SGDClassifier(loss='hinge', penalty='l2') # defaults\n",
    "    \n",
    "    else: raise ValueError('unknown algorithm: ' + str(alg))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_song_data_x, default_song_data_y = get_xy(default_song_data)\n",
    "default_vectorizer = get_default_vectorizer(default_song_data_x)\n",
    "\n",
    "clustered_song_data_x, clustered_song_data_y = get_xy(clustered_song_data)\n",
    "clustered_vectorizer = get_default_vectorizer(clustered_song_data_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        algorithm,\n",
    "        train_data,\n",
    "        vectorizer,\n",
    "        is_active,\n",
    "        active_init_n=None,\n",
    "        al_strat=None,\n",
    "        clusters=None\n",
    "    ):\n",
    "        '''\n",
    "        Class providing an interface for learning experiments.\n",
    "        \n",
    "        :param algorithm:       the name of the learning algorithm to use\n",
    "        :param train_data:      dict\n",
    "        :param vectorizer:      DictVectorizer, fit on train_data features\n",
    "        :param is_active:       bool\n",
    "        :param active_init_n:   number of instances to initialize active learner with\n",
    "        :param al_strat:        str, the active learning strategy to use\n",
    "                            \n",
    "        :return                 Classifier instance\n",
    "        '''\n",
    "        \n",
    "        if algorithm not in SUPPORTED_ALGS:\n",
    "            raise ValueError('Unsupported algorithm: ' + str(algorithm))\n",
    "        if is_active:\n",
    "            if al_strat not in AL_STRATS:\n",
    "                raise ValueError('Unsupported AL strategy: ' + str(al_strat))\n",
    "            if not active_init_n or active_init_n <= 0:\n",
    "                raise ValueError('Invalid active_init_n: ' + str(active_init_n))\n",
    "        if clusters and algorithm not in ONLINE_ALGS:\n",
    "            raise ValueError('Algorithm must be online if using cluster sampling.')\n",
    "        \n",
    "        self.algorithm = algorithm\n",
    "        self.train_data = train_data\n",
    "        self.vectorizer = vectorizer\n",
    "        self._is_active = is_active\n",
    "        self._is_online = algorithm in ONLINE_ALGS\n",
    "        self.strategy = al_strat\n",
    "        self.active_init_n = active_init_n\n",
    "        self.learner = get_default_learner(self.algorithm)\n",
    "        \n",
    "        self.clusters = clusters\n",
    "        self._uses_cluster_sampling = bool(clusters)\n",
    "        self.num_clusters = len(clusters) if clusters else None\n",
    "        \n",
    "        if not self.is_active():\n",
    "            self.x_train, self.y_train = get_xy(self.train_data)\n",
    "        \n",
    "        else:\n",
    "            if not self.uses_cluster_sampling():\n",
    "                self.unseen_ids = list(self.train_data.keys())\n",
    "            else:\n",
    "                self.unseen_ids = { i:list(self.clusters[i]) for i in range(self.num_clusters) }\n",
    "            \n",
    "            self.train_ids = set()\n",
    "            \n",
    "            self.active_learn(n=self.active_init_n, init=True)\n",
    "    \n",
    "    \n",
    "    def is_active(self):\n",
    "        return self._is_active\n",
    "    \n",
    "    \n",
    "    def is_online(self):\n",
    "        return self._is_online\n",
    "    \n",
    "    \n",
    "    def uses_cluster_sampling(self):\n",
    "        return self._uses_cluster_sampling\n",
    "    \n",
    "    \n",
    "    def transform(self, x):\n",
    "        '''\n",
    "        Vectorizes list of feature(s)\n",
    "        '''\n",
    "        return self.vectorizer.transform(x)\n",
    "    \n",
    "    \n",
    "    def fit(self):\n",
    "        '''\n",
    "        '''\n",
    "        if not self.is_active():\n",
    "            self.learner.fit(self.vectorizer.transform(self.x_train), self.y_train)\n",
    "        else:\n",
    "            self.active_learn()\n",
    "    \n",
    "    \n",
    "    def active_learn(self, n=1, init=False):\n",
    "        \n",
    "        if not self.uses_cluster_sampling():\n",
    "            sample_ids = self.al_sample(n, init)\n",
    "        else:\n",
    "            sample_ids = self.al_sample_clusters(n, init)\n",
    "        \n",
    "        if not self.is_online():\n",
    "            sample_x, sample_y = get_xy(self.train_data, ids=self.train_ids)\n",
    "            self.learner = get_default_learner(self.algorithm)\n",
    "            self.learner.fit(self.transform(sample_x), sample_y)\n",
    "        else:\n",
    "            sample_x, sample_y = get_xy(self.train_data, ids=sample_ids)\n",
    "            if init: self.learner.partial_fit(self.transform(sample_x), sample_y, CLASSES)\n",
    "            else: self.learner.partial_fit(self.transform(sample_x), sample_y)\n",
    "        \n",
    "    def al_sample(self, n, init):\n",
    "        \n",
    "        strategy = self.strategy if not init else 'random'\n",
    "        \n",
    "        if strategy == 'random':\n",
    "            \n",
    "            # Initial samples must contain all labels for offline learners.\n",
    "            # This ensures that happens.\n",
    "            if init and not self.is_online():\n",
    "                sampled_classes = set()\n",
    "                while len(sampled_classes) != len(CLASSES):\n",
    "                    sampled_classes = set()\n",
    "                    sample_ids = rand_sample(self.unseen_ids, n)\n",
    "                    sampled_classes = set()\n",
    "                    for id_ in sample_ids:\n",
    "                        sampled_classes.add(self.train_data[id_]['label'])\n",
    "            else:\n",
    "                sample_ids = rand_sample(self.unseen_ids, n)\n",
    "                \n",
    "            for song_id in sample_ids: self.unseen_ids.remove(song_id)\n",
    "        \n",
    "        elif strategy == 'uncertainty':\n",
    "            unseen_x, unseen_y = get_xy(self.train_data, ids=self.unseen_ids)\n",
    "            unseen_x_scores = self.learner.decision_function(self.transform(unseen_x))\n",
    "            \n",
    "            # get index of smallest absolute value (probability or distance from decision boundary)\n",
    "            # from unseen_x_scores, pop the corresponding entry from self.unseen_ids\n",
    "            sample_ids = [self.unseen_ids.pop(np.argmin(np.abs(unseen_x_scores)))]\n",
    "        \n",
    "        self.train_ids.update(sample_ids)\n",
    "        \n",
    "        return sample_ids\n",
    "    \n",
    "    \n",
    "    def al_sample_clusters(self, n, init):\n",
    "        \n",
    "        strategy = self.strategy if not init else 'random'\n",
    "        \n",
    "        sample_ids = []\n",
    "        \n",
    "        for i in range(self.num_clusters):\n",
    "            \n",
    "            if strategy == 'random':\n",
    "                new_sample_ids = rand_sample(self.unseen_ids[i], n)\n",
    "                for song_id in new_sample_ids: self.unseen_ids[i].remove(song_id)\n",
    "                sample_ids += new_sample_ids\n",
    "\n",
    "            elif strategy == 'uncertainty':\n",
    "                unseen_x, unseen_y = get_xy(self.train_data, ids=self.unseen_ids[i])\n",
    "                unseen_x_scores = self.learner.decision_function(self.transform(unseen_x))\n",
    "\n",
    "                # get index of smallest absolute value (probability or distance from decision boundary)\n",
    "                # from unseen_x_scores, pop the corresponding entry from self.unseen_ids\n",
    "                sample_ids.append(self.unseen_ids[i].pop(np.argmin(np.abs(unseen_x_scores))))\n",
    "            \n",
    "        self.train_ids.update(sample_ids)\n",
    "        \n",
    "        return sample_ids\n",
    "    \n",
    "    \n",
    "    def predict(self, x, learner=None):\n",
    "        return self.learner.predict(x)[0]\n",
    "    \n",
    "    \n",
    "    def validate(self, validation_data):\n",
    "        '''\n",
    "        Predicts on all instances in validation_data. Returns accuracy.\n",
    "        '''\n",
    "        x_test, y_test = get_xy(validation_data)\n",
    "        \n",
    "        correct = 0\n",
    "        for i in range(len(validation_data)):\n",
    "            if self.predict(self.transform([x_test[i]])) == y_test[i]: correct += 1\n",
    "        \n",
    "        return correct / len(validation_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_split(p=0.2):\n",
    "    # if discard: data = filter_features(data, discard)\n",
    "\n",
    "    default_trd, default_vd = split_data(default_song_data, p)\n",
    "    return default_trd, default_vd, { k:clustered_song_data[k] for k in default_trd }, { k:clustered_song_data[k] for k in default_vd }\n",
    "\n",
    "def get_train_clusters(clustered_train_data, all_clusters):\n",
    "    return { c:[_id for _id in all_clusters[c] if _id in clustered_train_data] for c in all_clusters}\n",
    "\n",
    "def run_active_experiment(train_data, validation_data, iterations, alg, init_n, strat='random', clusters=None):\n",
    "    '''\n",
    "    Trains :alg: on song_data (with or without filtering features), splits into train/validation,\n",
    "    and returns accuracy on validation data.\n",
    "    '''\n",
    "    \n",
    "    clf = Classifier(\n",
    "        alg,\n",
    "        train_data,\n",
    "        default_vectorizer,\n",
    "        True,\n",
    "        active_init_n=init_n,\n",
    "        al_strat=strat,\n",
    "        clusters=clusters\n",
    "    )\n",
    "    accuracies = [clf.validate(validation_data)]\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        clf.active_learn()\n",
    "        accuracies.append(clf.validate(validation_data))\n",
    "    \n",
    "    return accuracies\n",
    "\n",
    "def run_active_suite(train_data, validation_data):\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for alg in SUPPORTED_ALGS:\n",
    "        \n",
    "        results[alg] = {}\n",
    "        \n",
    "        for strat in AL_STRATS:\n",
    "            init_n = 5\n",
    "            accs = run_active_experiment(train_data, validation_data, 50, alg, init_n, strat=strat)\n",
    "            print(alg, 'accs w/', strat)\n",
    "            for i in range(len(accs)):\n",
    "                print(int(accs[i]*10000)/100, end='\\t')\n",
    "            print()\n",
    "            \n",
    "            results[alg][strat] = accs\n",
    "            \n",
    "    return results\n",
    "\n",
    "def run_clusters_suite(train_data, validation_data, train_clusters):\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for alg in ONLINE_ALGS:\n",
    "        \n",
    "        results[alg] = {}\n",
    "        \n",
    "        for strat in AL_STRATS:\n",
    "            \n",
    "            accs = run_active_experiment(\n",
    "                train_data, validation_data, 50//NUM_CLUSTERS, alg, len(train_clusters),\n",
    "                strat=strat, clusters=train_clusters\n",
    "            )\n",
    "            print(alg, 'accs w/', strat)\n",
    "            for i in range(len(accs)):\n",
    "                print(int(accs[i]*10000)/100, end='\\t')\n",
    "            print()\n",
    "            \n",
    "            results[alg][strat] = accs\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "Unclustered\n",
      "----\n",
      "svc accs w/ random\n",
      "66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t\n",
      "svc accs w/ uncertainty\n",
      "66.87\t66.87\t66.87\t67.92\t33.12\t69.93\t33.12\t75.54\t33.12\t80.1\t33.12\t82.99\t66.87\t81.68\t33.12\t84.92\t33.12\t85.53\t66.87\t85.45\t33.12\t86.32\t33.12\t87.02\t33.12\t85.36\t66.87\t86.76\t33.12\t85.8\t66.87\t86.67\t33.12\t86.67\t66.87\t86.67\t33.12\t86.85\t33.12\t85.71\t33.12\t84.13\t66.87\t86.5\t33.12\t85.88\t33.12\t84.13\t33.12\t82.29\t66.87\t\n",
      "lsvc accs w/ random\n",
      "66.95\t65.81\t65.73\t65.81\t66.34\t66.34\t66.34\t66.52\t66.52\t66.52\t67.92\t67.92\t67.83\t67.48\t73.53\t73.53\t73.0\t78.61\t78.61\t78.35\t78.35\t78.35\t79.49\t80.63\t80.63\t83.61\t85.53\t85.71\t85.45\t85.45\t85.88\t86.15\t86.32\t86.59\t86.59\t86.5\t86.59\t86.24\t86.24\t86.24\t86.41\t86.5\t86.06\t86.06\t86.06\t86.06\t86.06\t85.71\t85.71\t85.97\t86.15\t\n",
      "lsvc accs w/ uncertainty\n",
      "80.45\t80.01\t82.99\t85.36\t83.17\t85.27\t86.67\t85.8\t86.94\t86.41\t86.67\t86.85\t85.8\t85.97\t86.32\t87.02\t87.73\t87.9\t87.29\t87.2\t87.2\t86.32\t86.67\t86.94\t87.55\t87.64\t87.55\t87.73\t87.99\t88.16\t88.43\t87.9\t87.99\t88.25\t88.69\t88.51\t88.69\t88.78\t88.08\t88.25\t88.16\t88.16\t88.6\t88.51\t88.6\t88.78\t88.51\t88.43\t88.34\t88.43\t88.25\t\n",
      "sgd accs w/ random\n",
      "67.13\t67.13\t67.13\t35.67\t68.88\t68.88\t68.88\t68.88\t37.24\t37.24\t80.63\t40.75\t78.7\t78.7\t78.7\t78.7\t78.7\t78.7\t78.7\t78.7\t78.7\t78.7\t78.7\t78.7\t78.7\t78.7\t78.7\t78.7\t57.05\t57.05\t80.28\t56.44\t80.89\t80.89\t80.89\t80.89\t73.44\t73.44\t83.61\t83.61\t76.51\t76.51\t76.51\t76.51\t76.51\t76.51\t76.51\t76.51\t84.75\t84.75\t84.75\t\n",
      "sgd accs w/ uncertainty\n",
      "66.87\t66.87\t66.87\t66.87\t66.87\t68.27\t50.56\t33.12\t47.76\t86.41\t47.85\t33.12\t48.02\t86.15\t47.85\t33.82\t33.12\t33.21\t33.12\t33.12\t41.63\t33.21\t39.26\t57.66\t86.41\t55.65\t85.88\t75.98\t87.9\t60.73\t87.64\t61.17\t87.81\t83.61\t87.02\t59.77\t84.22\t85.45\t84.83\t86.5\t87.46\t85.53\t87.9\t60.99\t87.9\t60.47\t87.81\t86.85\t87.29\t87.64\t80.98\t\n",
      "----\n",
      "Clustered\n",
      "----\n",
      "svc accs w/ random\n",
      "33.12\t78.7\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t66.87\t\n",
      "svc accs w/ uncertainty\n",
      "33.12\t33.12\t33.12\t46.27\t66.87\t56.44\t66.87\t84.57\t66.87\t84.83\t33.12\t87.37\t66.87\t85.27\t33.12\t87.02\t33.12\t84.92\t66.87\t87.46\t33.12\t85.18\t66.87\t87.73\t33.12\t83.52\t66.87\t87.64\t33.12\t86.32\t66.87\t88.34\t33.12\t83.87\t66.87\t88.16\t66.87\t87.81\t33.12\t88.43\t33.12\t86.15\t66.87\t88.95\t33.12\t83.78\t66.87\t88.43\t66.87\t88.25\t66.87\t\n",
      "lsvc accs w/ random\n",
      "66.87\t66.87\t66.87\t68.71\t81.68\t82.64\t83.08\t83.52\t83.87\t84.57\t83.96\t84.92\t84.92\t85.01\t85.45\t85.27\t85.18\t85.27\t86.32\t86.24\t86.15\t86.32\t86.06\t86.67\t86.59\t86.5\t86.59\t86.41\t86.85\t87.37\t87.37\t87.37\t87.55\t87.64\t87.73\t87.64\t87.64\t87.81\t87.81\t87.9\t87.81\t88.16\t87.81\t87.64\t86.5\t86.41\t86.94\t86.94\t87.37\t87.2\t87.46\t\n",
      "lsvc accs w/ uncertainty\n",
      "81.94\t86.24\t80.1\t87.37\t86.15\t86.67\t86.85\t86.94\t86.94\t86.32\t87.9\t88.34\t88.08\t88.69\t88.16\t88.86\t88.6\t87.9\t88.86\t88.43\t88.69\t88.6\t88.78\t88.34\t88.95\t88.78\t88.69\t88.69\t88.51\t88.86\t89.04\t89.3\t89.3\t89.04\t89.3\t89.3\t89.39\t89.3\t89.39\t89.57\t89.48\t89.65\t89.65\t89.57\t89.57\t89.65\t89.57\t89.39\t89.39\t89.57\t89.48\t\n",
      "sgd accs w/ random\n",
      "74.23\t74.23\t74.23\t66.87\t66.87\t66.87\t66.87\t66.87\t85.71\t85.71\t85.71\t67.04\t82.03\t82.03\t66.95\t66.95\t66.95\t66.95\t66.95\t66.95\t86.59\t86.59\t33.21\t86.94\t86.94\t86.94\t86.94\t86.94\t86.94\t86.94\t86.94\t66.95\t66.95\t77.91\t77.91\t77.91\t77.91\t77.91\t34.88\t74.67\t74.67\t35.23\t78.26\t78.26\t36.72\t36.72\t36.72\t81.68\t81.68\t81.68\t36.45\t\n",
      "sgd accs w/ uncertainty\n",
      "67.74\t47.94\t69.41\t57.31\t85.45\t67.04\t86.24\t68.36\t83.96\t75.19\t85.62\t80.54\t87.2\t85.27\t84.22\t85.18\t87.64\t49.07\t85.18\t84.92\t87.99\t47.94\t88.25\t86.24\t88.51\t54.42\t81.68\t87.55\t74.93\t87.81\t83.34\t87.55\t80.54\t87.9\t85.45\t87.81\t84.22\t87.99\t87.37\t65.2\t89.13\t56.96\t85.01\t88.16\t81.41\t88.16\t88.34\t88.16\t87.2\t88.86\t84.04\t\n",
      "\n",
      "----\n",
      "Clustered w/ Cluster Sampling\n",
      "----\n",
      "sgd accs w/ random\n",
      "84.66\t84.66\t66.87\t67.57\t83.61\t83.61\t81.94\t86.94\t87.55\t87.55\t87.55\t57.05\t67.39\t67.39\t88.25\t88.25\t88.25\t\n",
      "sgd accs w/ uncertainty\n",
      "67.3\t82.38\t67.39\t84.83\t67.57\t66.95\t67.57\t67.83\t86.06\t51.62\t86.85\t55.47\t85.27\t88.08\t73.88\t78.08\t83.26\t\n"
     ]
    }
   ],
   "source": [
    "default_training_data, default_validation_data, clustered_training_data, clustered_validation_data = get_experiment_split()\n",
    "training_clusters = get_train_clusters(clustered_training_data, songs_by_cluster[NUM_CLUSTERS])\n",
    "print('----\\nUnclustered\\n----')\n",
    "active_unclustered_results = run_active_suite(default_training_data, default_validation_data)\n",
    "print('----\\nClustered\\n----')\n",
    "active_clustered_results = run_active_suite(clustered_training_data, clustered_validation_data)\n",
    "print('\\n----\\nClustered w/ Cluster Sampling\\n----')\n",
    "active_cluster_sampled_results = run_clusters_suite(clustered_training_data, clustered_validation_data, training_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(alg, data, p, discard=None):\n",
    "    '''\n",
    "    Trains :alg: on song_data (with or without filtering features), splits into train/validation,\n",
    "    and returns accuracy on validation data.\n",
    "    '''\n",
    "    if discard: data = filter_features(data, discard)\n",
    "    \n",
    "    train_data, validation_data = split_data(data, p)\n",
    "    \n",
    "    clf = Classifier(alg, train_data, default_vectorizer, False)\n",
    "    clf.fit()\n",
    "    \n",
    "    return clf, train_data, validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svc acc: 0.6827344434706398\n",
      "lsvc acc: 0.9149868536371604\n",
      "sgd acc: 0.9149868536371604\n",
      "\n",
      "{'svc': {'random': [0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178], 'uncertainty': [0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6792287467134093, 0.3312883435582822, 0.6993865030674846, 0.3312883435582822, 0.7554776511831727, 0.3312883435582822, 0.8010517090271692, 0.3312883435582822, 0.8299737072743207, 0.6687116564417178, 0.8168273444347064, 0.3312883435582822, 0.8492550394390885, 0.3312883435582822, 0.8553900087642419, 0.6687116564417178, 0.8545135845749343, 0.3312883435582822, 0.8632778264680105, 0.3312883435582822, 0.8702892199824716, 0.3312883435582822, 0.8536371603856266, 0.6687116564417178, 0.8676599474145487, 0.3312883435582822, 0.8580192813321648, 0.6687116564417178, 0.866783523225241, 0.3312883435582822, 0.866783523225241, 0.6687116564417178, 0.866783523225241, 0.3312883435582822, 0.8685363716038562, 0.3312883435582822, 0.8571428571428571, 0.3312883435582822, 0.8413672217353199, 0.6687116564417178, 0.8650306748466258, 0.3312883435582822, 0.8588957055214724, 0.3312883435582822, 0.8413672217353199, 0.3312883435582822, 0.8229623137598597, 0.6687116564417178]}, 'lsvc': {'random': [0.6695880806310254, 0.6581945661700263, 0.6573181419807187, 0.6581945661700263, 0.663453111305872, 0.663453111305872, 0.663453111305872, 0.6652059596844873, 0.6652059596844873, 0.6652059596844873, 0.6792287467134093, 0.6792287467134093, 0.6783523225241017, 0.6748466257668712, 0.7353198948290973, 0.7353198948290973, 0.7300613496932515, 0.7861524978089395, 0.7861524978089395, 0.7835232252410167, 0.7835232252410167, 0.7835232252410167, 0.7949167397020158, 0.8063102541630149, 0.8063102541630149, 0.8361086765994742, 0.8553900087642419, 0.8571428571428571, 0.8545135845749343, 0.8545135845749343, 0.8588957055214724, 0.8615249780893953, 0.8632778264680105, 0.8659070990359334, 0.8659070990359334, 0.8650306748466258, 0.8659070990359334, 0.8624014022787029, 0.8624014022787029, 0.8624014022787029, 0.8641542506573181, 0.8650306748466258, 0.8606485539000877, 0.8606485539000877, 0.8606485539000877, 0.8606485539000877, 0.8606485539000877, 0.8571428571428571, 0.8571428571428571, 0.85977212971078, 0.8615249780893953], 'uncertainty': [0.8045574057843996, 0.8001752848378615, 0.8299737072743207, 0.8536371603856266, 0.8317265556529361, 0.852760736196319, 0.866783523225241, 0.8580192813321648, 0.8694127957931639, 0.8641542506573181, 0.866783523225241, 0.8685363716038562, 0.8580192813321648, 0.85977212971078, 0.8632778264680105, 0.8702892199824716, 0.8773006134969326, 0.8790534618755478, 0.8729184925503944, 0.8720420683610868, 0.8720420683610868, 0.8632778264680105, 0.866783523225241, 0.8694127957931639, 0.8755477651183172, 0.8764241893076249, 0.8755477651183172, 0.8773006134969326, 0.8799298860648553, 0.8816827344434707, 0.8843120070113936, 0.8790534618755478, 0.8799298860648553, 0.8825591586327782, 0.8869412795793163, 0.8851884312007011, 0.8869412795793163, 0.887817703768624, 0.880806310254163, 0.8825591586327782, 0.8816827344434707, 0.8816827344434707, 0.8860648553900088, 0.8851884312007011, 0.8860648553900088, 0.887817703768624, 0.8851884312007011, 0.8843120070113936, 0.8834355828220859, 0.8843120070113936, 0.8825591586327782]}, 'sgd': {'random': [0.6713409290096407, 0.6713409290096407, 0.6713409290096407, 0.3567046450482033, 0.6888694127957932, 0.6888694127957932, 0.6888694127957932, 0.6888694127957932, 0.37248028045574055, 0.37248028045574055, 0.8063102541630149, 0.4075372480280456, 0.7870289219982471, 0.7870289219982471, 0.7870289219982471, 0.7870289219982471, 0.7870289219982471, 0.7870289219982471, 0.7870289219982471, 0.7870289219982471, 0.7870289219982471, 0.7870289219982471, 0.7870289219982471, 0.7870289219982471, 0.7870289219982471, 0.7870289219982471, 0.7870289219982471, 0.7870289219982471, 0.5705521472392638, 0.5705521472392638, 0.8028045574057844, 0.5644171779141104, 0.8089395267309377, 0.8089395267309377, 0.8089395267309377, 0.8089395267309377, 0.7344434706397897, 0.7344434706397897, 0.8361086765994742, 0.8361086765994742, 0.7651183172655566, 0.7651183172655566, 0.7651183172655566, 0.7651183172655566, 0.7651183172655566, 0.7651183172655566, 0.7651183172655566, 0.7651183172655566, 0.8475021910604733, 0.8475021910604733, 0.8475021910604733], 'uncertainty': [0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6827344434706398, 0.5056967572304996, 0.3312883435582822, 0.4776511831726556, 0.8641542506573181, 0.4785276073619632, 0.3312883435582822, 0.4802804557405784, 0.8615249780893953, 0.4785276073619632, 0.3382997370727432, 0.3312883435582822, 0.33216476774758985, 0.3312883435582822, 0.3312883435582822, 0.4163014899211218, 0.33216476774758985, 0.39263803680981596, 0.5766871165644172, 0.8641542506573181, 0.5565293602103418, 0.8588957055214724, 0.7598597721297108, 0.8790534618755478, 0.6073619631901841, 0.8764241893076249, 0.6117440841367222, 0.8781770376862401, 0.8361086765994742, 0.8702892199824716, 0.5977212971078002, 0.8422436459246275, 0.8545135845749343, 0.8483786152497809, 0.8650306748466258, 0.8746713409290097, 0.8553900087642419, 0.8790534618755478, 0.6099912357581069, 0.8790534618755478, 0.6047326906222612, 0.8781770376862401, 0.8685363716038562, 0.8729184925503944, 0.8764241893076249, 0.8098159509202454]}}\n",
      "{'svc': {'random': [0.3312883435582822, 0.7870289219982471, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178], 'uncertainty': [0.3312883435582822, 0.3312883435582822, 0.3312883435582822, 0.46275197195442597, 0.6687116564417178, 0.5644171779141104, 0.6687116564417178, 0.845749342681858, 0.6687116564417178, 0.8483786152497809, 0.3312883435582822, 0.873794916739702, 0.6687116564417178, 0.852760736196319, 0.3312883435582822, 0.8702892199824716, 0.3312883435582822, 0.8492550394390885, 0.6687116564417178, 0.8746713409290097, 0.3312883435582822, 0.8518843120070114, 0.6687116564417178, 0.8773006134969326, 0.3312883435582822, 0.8352322524101665, 0.6687116564417178, 0.8764241893076249, 0.3312883435582822, 0.8632778264680105, 0.6687116564417178, 0.8834355828220859, 0.3312883435582822, 0.838737949167397, 0.6687116564417178, 0.8816827344434707, 0.6687116564417178, 0.8781770376862401, 0.3312883435582822, 0.8843120070113936, 0.3312883435582822, 0.8615249780893953, 0.6687116564417178, 0.8895705521472392, 0.3312883435582822, 0.8378615249780894, 0.6687116564417178, 0.8843120070113936, 0.6687116564417178, 0.8825591586327782, 0.6687116564417178]}, 'lsvc': {'random': [0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6871165644171779, 0.8168273444347064, 0.8264680105170903, 0.8308501314636284, 0.8352322524101665, 0.838737949167397, 0.845749342681858, 0.8396143733567046, 0.8492550394390885, 0.8492550394390885, 0.8501314636283961, 0.8545135845749343, 0.852760736196319, 0.8518843120070114, 0.852760736196319, 0.8632778264680105, 0.8624014022787029, 0.8615249780893953, 0.8632778264680105, 0.8606485539000877, 0.866783523225241, 0.8659070990359334, 0.8650306748466258, 0.8659070990359334, 0.8641542506573181, 0.8685363716038562, 0.873794916739702, 0.873794916739702, 0.873794916739702, 0.8755477651183172, 0.8764241893076249, 0.8773006134969326, 0.8764241893076249, 0.8764241893076249, 0.8781770376862401, 0.8781770376862401, 0.8790534618755478, 0.8781770376862401, 0.8816827344434707, 0.8781770376862401, 0.8764241893076249, 0.8650306748466258, 0.8641542506573181, 0.8694127957931639, 0.8694127957931639, 0.873794916739702, 0.8720420683610868, 0.8746713409290097], 'uncertainty': [0.8194566170026293, 0.8624014022787029, 0.8010517090271692, 0.873794916739702, 0.8615249780893953, 0.866783523225241, 0.8685363716038562, 0.8694127957931639, 0.8694127957931639, 0.8632778264680105, 0.8790534618755478, 0.8834355828220859, 0.880806310254163, 0.8869412795793163, 0.8816827344434707, 0.8886941279579317, 0.8860648553900088, 0.8790534618755478, 0.8886941279579317, 0.8843120070113936, 0.8869412795793163, 0.8860648553900088, 0.887817703768624, 0.8834355828220859, 0.8895705521472392, 0.887817703768624, 0.8869412795793163, 0.8869412795793163, 0.8851884312007011, 0.8886941279579317, 0.8904469763365469, 0.8930762489044698, 0.8930762489044698, 0.8904469763365469, 0.8930762489044698, 0.8930762489044698, 0.8939526730937774, 0.8930762489044698, 0.8939526730937774, 0.8957055214723927, 0.894829097283085, 0.8965819456617002, 0.8965819456617002, 0.8957055214723927, 0.8957055214723927, 0.8965819456617002, 0.8957055214723927, 0.8939526730937774, 0.8939526730937774, 0.8957055214723927, 0.894829097283085]}, 'sgd': {'random': [0.7423312883435583, 0.7423312883435583, 0.7423312883435583, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.6687116564417178, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.670464504820333, 0.820333041191937, 0.820333041191937, 0.6695880806310254, 0.6695880806310254, 0.6695880806310254, 0.6695880806310254, 0.6695880806310254, 0.6695880806310254, 0.8659070990359334, 0.8659070990359334, 0.33216476774758985, 0.8694127957931639, 0.8694127957931639, 0.8694127957931639, 0.8694127957931639, 0.8694127957931639, 0.8694127957931639, 0.8694127957931639, 0.8694127957931639, 0.6695880806310254, 0.6695880806310254, 0.7791411042944786, 0.7791411042944786, 0.7791411042944786, 0.7791411042944786, 0.7791411042944786, 0.3488168273444347, 0.7467134092900964, 0.7467134092900964, 0.3523225241016652, 0.782646801051709, 0.782646801051709, 0.3672217353198948, 0.3672217353198948, 0.3672217353198948, 0.8168273444347064, 0.8168273444347064, 0.8168273444347064, 0.36459246275197194], 'uncertainty': [0.677475898334794, 0.4794040315512708, 0.694127957931639, 0.5731814198071867, 0.8545135845749343, 0.670464504820333, 0.8624014022787029, 0.6836108676599474, 0.8396143733567046, 0.7519719544259421, 0.8562664329535495, 0.8054338299737073, 0.8720420683610868, 0.852760736196319, 0.8422436459246275, 0.8518843120070114, 0.8764241893076249, 0.49079754601226994, 0.8518843120070114, 0.8492550394390885, 0.8799298860648553, 0.4794040315512708, 0.8825591586327782, 0.8624014022787029, 0.8851884312007011, 0.544259421560035, 0.8168273444347064, 0.8755477651183172, 0.7493426818580193, 0.8781770376862401, 0.8334794040315513, 0.8755477651183172, 0.8054338299737073, 0.8790534618755478, 0.8545135845749343, 0.8781770376862401, 0.8422436459246275, 0.8799298860648553, 0.873794916739702, 0.6520595968448729, 0.8913234005258545, 0.5696757230499562, 0.8501314636283961, 0.8816827344434707, 0.8141980718667835, 0.8816827344434707, 0.8834355828220859, 0.8816827344434707, 0.8720420683610868, 0.8886941279579317, 0.8404907975460123]}}\n",
      "{'sgd': {'random': [0.8466257668711656, 0.8466257668711656, 0.6687116564417178, 0.6757230499561788, 0.8361086765994742, 0.8361086765994742, 0.8194566170026293, 0.8694127957931639, 0.8755477651183172, 0.8755477651183172, 0.8755477651183172, 0.5705521472392638, 0.6739702015775635, 0.6739702015775635, 0.8825591586327782, 0.8825591586327782, 0.8825591586327782], 'uncertainty': [0.6730937773882559, 0.8238387379491674, 0.6739702015775635, 0.8483786152497809, 0.6757230499561788, 0.6695880806310254, 0.6757230499561788, 0.6783523225241017, 0.8606485539000877, 0.516213847502191, 0.8685363716038562, 0.5547765118317266, 0.852760736196319, 0.880806310254163, 0.7388255915863278, 0.7808939526730938, 0.8326029798422436]}}\n"
     ]
    }
   ],
   "source": [
    "test_clf, test_td, test_vd = run_benchmark('svc', default_song_data, 0.2)\n",
    "svc_bench_acc = test_clf.validate(test_vd)\n",
    "print('svc acc:', svc_bench_acc)\n",
    "test_clf, test_td, test_vd = run_benchmark('lsvc', default_song_data, 0.2)\n",
    "lsvc_bench_acc = test_clf.validate(test_vd)\n",
    "print('lsvc acc:', lsvc_bench_acc)\n",
    "test_clf, test_td, test_vd = run_benchmark('sgd', default_song_data, 0.2)\n",
    "sgd_bench_acc = test_clf.validate(test_vd)\n",
    "print('sgd acc:', sgd_bench_acc)\n",
    "# print()\n",
    "# print(active_unclustered_results)\n",
    "# print(active_clustered_results)\n",
    "# print(active_cluster_sampled_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
